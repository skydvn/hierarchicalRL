{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hierarchicalRL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6DxtZcUUXEMM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "LLnkhj8GQVvJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9gbHyTbDQ1hL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754c13cc-1b3c-455c-c8c6-ac1d7483d269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/Shareddrives/Duong-LongWarwick/FARL/StateAbstraction/hierarchical_DQN\n",
            "agents\t       experiment_logs\t\t       README.md\n",
            "clustering.py  hierarchicalRL.ipynb\t       results\n",
            "clusters       hierarchicalRL_reference.ipynb  train_dqn.py\n",
            "clusters_6     make_plots.py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/Shareddrives/Duong-LongWarwick/FARL/StateAbstraction/hierarchical_DQN\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yphAISF4KFPh",
        "outputId": "ad16da67-d022-4a4b-db4a-804d3223f80e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.21.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.46.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import "
      ],
      "metadata": {
        "id": "BSV6VyJXQcso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# import clustering\n",
        "# import dqn\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "RP67s5AlWdiP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent"
      ],
      "metadata": {
        "id": "D7A1CMfcQing"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### replay_buffer.py"
      ],
      "metadata": {
        "id": "zeSKH0w6XVgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size, init_size, batch_size):\n",
        "        self.max_size = max_size\n",
        "        self.init_size = init_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.states = np.array([None] * self.max_size)\n",
        "        self.actions = np.array([None] * self.max_size)\n",
        "        self.rewards = np.array([None] * self.max_size)\n",
        "        self.next_states = np.array([None] * self.max_size)\n",
        "        self.terminals = np.array([None] * self.max_size)\n",
        "\n",
        "        self.curr_pointer = 0\n",
        "        self.curr_size = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, terminal):\n",
        "        self.states[self.curr_pointer] = np.squeeze(state)\n",
        "        self.actions[self.curr_pointer] = action\n",
        "        self.rewards[self.curr_pointer] = reward\n",
        "        self.next_states[self.curr_pointer] = np.squeeze(next_state)\n",
        "        self.terminals[self.curr_pointer] = terminal\n",
        "\n",
        "        self.curr_pointer += 1\n",
        "        self.curr_size = min(self.max_size, self.curr_size + 1)\n",
        "        # If replay buffer is full, set current pointer to be at the beginning of the buffer.\n",
        "        if self.curr_pointer >= self.max_size:\n",
        "            self.curr_pointer -= self.max_size\n",
        "\n",
        "    def sample(self):\n",
        "        if self.curr_size < self.init_size:\n",
        "            return [], [], [], [], []\n",
        "        sample_indices = []\n",
        "\n",
        "        # Ensure that the most recent transition is in the returned batch.\n",
        "        sample_indices.append(self.curr_pointer - 1)\n",
        "        for i in range(self.batch_size - 1):\n",
        "            sample_indices.append(random.randint(0, self.curr_size - 1))\n",
        "\n",
        "        returned_states = []\n",
        "        returned_actions = []\n",
        "        returned_rewards = []\n",
        "        returned_next_states = []\n",
        "        returned_terminals = []\n",
        "\n",
        "        for i in range(len(sample_indices)):\n",
        "            index = sample_indices[i]\n",
        "            returned_states.append(self.states[index])\n",
        "            returned_actions.append(self.actions[index])\n",
        "            returned_rewards.append(self.rewards[index])\n",
        "            returned_next_states.append(self.next_states[index])\n",
        "            returned_terminals.append(self.terminals[index])\n",
        "\n",
        "        return np.array(returned_states), np.array(returned_actions), np.array(\n",
        "            returned_rewards), np.array(returned_next_states), np.array(returned_terminals)\n",
        "        # return self.states[sample_indices], self.actions[sample_indices], self.rewards[sample_indices], self.next_states[sample_indices], self.terminals[sample_indices]\n"
      ],
      "metadata": {
        "id": "DQGXfBHvXb1S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dqn.py"
      ],
      "metadata": {
        "id": "wnApLig_QQOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DqnAgent(object):\n",
        "\n",
        "    # Discount factor for future rewards.\n",
        "    DISCOUNT = 0.99\n",
        "    # Max size of the replay buffer.\n",
        "    REPLAY_MEMORY_SIZE = 500000\n",
        "    # Batch size for updates from the replay buffer.\n",
        "    BATCH_SIZE = 32\n",
        "    # Initial size of replay memory prior to beginning sampling batches.\n",
        "    REPLAY_MEMORY_INIT_SIZE = 5000\n",
        "    # Update the target network every TARGET_UPDATE timesteps.\n",
        "    TARGET_UPDATE = 1000 #10000\n",
        "\n",
        "    def __init__(self, sess=None, learning_rate=0.00025, state_dims=[], num_actions=0,\n",
        "        epsilon_start=1.0, epsilon_end=0.1, epsilon_decay_steps=50000, replay_memory_init_size=None,\n",
        "        target_update=None):\n",
        "\n",
        "        self._learning_rate = learning_rate\n",
        "        self._state_dims = state_dims\n",
        "        self._num_actions = num_actions\n",
        "\n",
        "        self._epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
        "        self._epsilon_decay_steps = epsilon_decay_steps\n",
        "\n",
        "        if replay_memory_init_size is not None:\n",
        "            self.REPLAY_MEMORY_INIT_SIZE = replay_memory_init_size\n",
        "\n",
        "        if target_update is not None:\n",
        "            self.TARGET_UPDATE = target_update\n",
        "\n",
        "        self._replay_buffer = ReplayBuffer(\n",
        "            self.REPLAY_MEMORY_SIZE,\n",
        "            self.REPLAY_MEMORY_INIT_SIZE,\n",
        "            self.BATCH_SIZE)\n",
        "\n",
        "        self._current_time_step = 0\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            self._construct_graph()\n",
        "            self._saver = tf.train.Saver()\n",
        "            if sess is None:\n",
        "                self.sess = tf.Session()\n",
        "            else:\n",
        "                self.sess = sess\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def _q_network(self, state):\n",
        "        print(\"Controller!\")\n",
        "        # Three convolutional layers\n",
        "        # conv1 = tf.contrib.layers.conv2d(\n",
        "        #    state, 32, 8, 4, activation_fn=tf.nn.relu)\n",
        "        # conv2 = tf.contrib.layers.conv2d(\n",
        "        #    conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
        "        # conv3 = tf.contrib.layers.conv2d(\n",
        "        #    conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
        "\n",
        "        layer1 = tf.contrib.layers.fully_connected(state, 64, activation_fn=tf.nn.relu)\n",
        "        # layer2 = tf.contrib.layers.fully_connected(layer1, 100, activation_fn=tf.nn.sigmoid)\n",
        "        # layer3 = tf.contrib.layers.fully_connected(layer2, 100, activation_fn=tf.nn.relu)\n",
        "        q_values = tf.contrib.layers.fully_connected(layer1, self._num_actions, activation_fn=None)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # flattened = tf.contrib.layers.flatten(conv3)\n",
        "        # fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
        "        # q_values = tf.contrib.layers.fully_connected(fc1, self._num_actions)\n",
        "\n",
        "        return q_values\n",
        "\n",
        "    def _construct_graph(self):\n",
        "        shape=[None]\n",
        "        for dim in self._state_dims:\n",
        "            shape.append(dim)\n",
        "        self._state = tf.placeholder(shape=shape, dtype=tf.float32)\n",
        "\n",
        "        with tf.variable_scope('q_network'):\n",
        "            self._q_values = self._q_network(self._state)\n",
        "        with tf.variable_scope('target_q_network'):\n",
        "            self._target_q_values = self._q_network(self._state)\n",
        "        with tf.variable_scope('q_network_update'):\n",
        "            self._picked_actions = tf.placeholder(shape=[None, 2], dtype=tf.int32)\n",
        "            self._td_targets = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "            self._q_values_pred = tf.gather_nd(self._q_values, self._picked_actions)\n",
        "            # self._losses = tf.square(self._q_values_pred, self._td_targets)\n",
        "            self._losses = clipped_error(self._q_values_pred - self._td_targets)\n",
        "            self._loss = tf.reduce_mean(self._losses)\n",
        "\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self._learning_rate)\n",
        "            # self.optimizer = tf.train.RMSPropOptimizer(self._learning_rate, 0.99, 0.0, 1e-6)\n",
        "            # self.optimizer = tf.train.AdamOptimizer(0.0001)\n",
        "            # self.optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
        "            grads_and_vars = self.optimizer.compute_gradients(self._loss, tf.trainable_variables())\n",
        "            grads = [gv[0] for gv in grads_and_vars]\n",
        "            params = [gv[1] for gv in grads_and_vars]\n",
        "\n",
        "            grads = tf.clip_by_global_norm(grads, 5.0)[0]\n",
        "\n",
        "            # clipped_grads_and_vars = [(\n",
        "            #    tf.clip_by_norm(grad, 5.0), var) for grad, var in grads_and_vars]\n",
        "            clipped_grads_and_vars = list(zip(grads, params))\n",
        "            print(clipped_grads_and_vars)\n",
        "            self.train_op = self.optimizer.apply_gradients(clipped_grads_and_vars,\n",
        "                global_step=tf.contrib.framework.get_global_step())\n",
        "            print(\"4\")\n",
        "            # self.train_op = self.optimizer.minimize(self._loss,\n",
        "            #    global_step=tf.contrib.framework.get_global_step())\n",
        "        with tf.name_scope('target_network_update'):\n",
        "            q_network_params = [t for t in tf.trainable_variables() if t.name.startswith(\n",
        "                'q_network')]\n",
        "            q_network_params = sorted(q_network_params, key=lambda v: v.name)\n",
        "\n",
        "            target_q_network_params = [t for t in tf.trainable_variables() if t.name.startswith(\n",
        "                'target_q_network')]\n",
        "            target_q_network_params = sorted(target_q_network_params, key=lambda v: v.name)\n",
        "\n",
        "            self.target_update_ops = []\n",
        "            for e1_v, e2_v in zip(q_network_params, target_q_network_params):\n",
        "                op = e2_v.assign(e1_v)\n",
        "                self.target_update_ops.append(op)\n",
        "\n",
        "    def sample(self, state):\n",
        "        self._current_time_step += 1\n",
        "        q_values = self.sess.run(self._q_values, {self._state: state})\n",
        "\n",
        "        epsilon = self._epsilons[min(self._current_time_step, self._epsilon_decay_steps - 1)]\n",
        "\n",
        "        e = random.random()\n",
        "        if e < epsilon:\n",
        "            return random.randint(0, self._num_actions - 1)\n",
        "        else:\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def best_action(self, state):\n",
        "        q_values = self.sess.run(self._q_values, {self._state: state})\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def store(self, state, action, reward, next_state, terminal, eval=False, curr_reward=False):\n",
        "        if not eval:\n",
        "            self._replay_buffer.add(state, action, reward, next_state, terminal)\n",
        "\n",
        "    def update(self):\n",
        "        states, actions, rewards, next_states, terminals = self._replay_buffer.sample()\n",
        "        '''\n",
        "        print \"Update!\"\n",
        "        print states\n",
        "        print actions\n",
        "        print rewards\n",
        "        print terminals\n",
        "        print \"\"\n",
        "        '''\n",
        "\n",
        "        actions = np.array(list(zip(np.arange(len(actions)), actions)))\n",
        "\n",
        "        if len(states) > 0:\n",
        "            next_states_q_values = self.sess.run(self._target_q_values, {self._state: next_states})\n",
        "\n",
        "            # print \"Next States Q Values:\"\n",
        "            # print next_states_q_values\n",
        "\n",
        "            next_states_max_q_values = np.max(next_states_q_values, axis=1)\n",
        "\n",
        "            td_targets = rewards + (1 - terminals) * self.DISCOUNT * next_states_max_q_values\n",
        "\n",
        "            feed_dict = {self._state: states,\n",
        "                         self._picked_actions: actions,\n",
        "                         self._td_targets: td_targets}\n",
        "\n",
        "            _ = self.sess.run(self.train_op, feed_dict=feed_dict)\n",
        "\n",
        "        # Update the target q-network.\n",
        "        if not self._current_time_step % self.TARGET_UPDATE:\n",
        "            # print self._current_time_step\n",
        "            # print self._epsilons[min(self._current_time_step, self._epsilon_decay_steps - 1)]\n",
        "            # print \"Updating target!\"\n",
        "            self.sess.run(self.target_update_ops)\n",
        "\n",
        "def clipped_error(x):\n",
        "    return tf.where(tf.abs(x) < 1.0, 0.5 * tf.square(x), tf.abs(x) - 0.5)\n",
        "\n",
        "def compute_gradients(tensor, var_list):\n",
        "  grads = tf.gradients(tensor, var_list)\n",
        "  return [grad if grad is not None else tf.zeros_like(var)\n",
        "          for var, grad in zip(var_list, grads)]"
      ],
      "metadata": {
        "id": "EnxLEcvcQR5I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Qlearning.py"
      ],
      "metadata": {
        "id": "JAJdVaCzXUDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent(object):\n",
        "    \"\"\"Implementation of tabular Q-learning.\"\"\"\n",
        "\n",
        "    DISCOUNT = 0.95\n",
        "\n",
        "    def __init__(self, num_states, num_actions, learning_rate, epsilon=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros((self.num_states, self.num_actions))\n",
        "        self.curr_transition = None\n",
        "        self.epsilon_decay_steps = 5000\n",
        "        self.epsilons = np.linspace(1.0, 0.01, self.epsilon_decay_steps)\n",
        "        self.curr_time_step = 0\n",
        "\n",
        "    def get_avaiable_actions(self, state):\n",
        "        state_index = np.where(np.squeeze(state) == 1)[0][0]\n",
        "        available_actions = []\n",
        "        for i in range(len(state)):\n",
        "            if i != state_index:\n",
        "                available_actions.append(i)\n",
        "\n",
        "        return available_actions\n",
        "\n",
        "    def compute_state_index(self, state):\n",
        "        if np.sum(state) == 1:\n",
        "            state_index = np.where(np.squeeze(state) == 1)[0][0]\n",
        "            return state_index\n",
        "        else:\n",
        "            # State vector contains an extra bit at the end.\n",
        "            state_index = np.where(np.squeeze(state) == 1)[0][0]\n",
        "            return self.num_actions + state_index\n",
        "\n",
        "    def sample(self, state):\n",
        "        state_index = self.compute_state_index(state)\n",
        "        q_values = self.q_table[state_index]\n",
        "        self.curr_time_step += 1\n",
        "        e = self.epsilons[min(self.curr_time_step, self.epsilon_decay_steps - 1)]\n",
        "        e = random.random()\n",
        "        if e < self.epsilon:\n",
        "            return random.randint(0, self.num_actions - 1)\n",
        "        else:\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def best_action(self, state):\n",
        "        state_index = self.compute_state_index(state)\n",
        "        q_values = self.q_table[state_index]\n",
        "\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def store(self, state, action, reward, next_state, terminal, eval, curr_reward):\n",
        "        if not eval:\n",
        "            self.curr_transition = [state, action, reward, next_state, terminal]\n",
        "            self.curr_reward = curr_reward\n",
        "\n",
        "    def update(self):\n",
        "        state = self.curr_transition[0]\n",
        "        action = self.curr_transition[1]\n",
        "        reward = self.curr_transition[2]\n",
        "        next_state = self.curr_transition[3]\n",
        "        terminal = self.curr_transition[4]\n",
        "\n",
        "        state_index = self.compute_state_index(state)\n",
        "        next_state_index = self.compute_state_index(next_state)\n",
        "\n",
        "        td_target = reward + (1 - terminal) * self.DISCOUNT * np.max(self.q_table[next_state_index])\n",
        "\n",
        "        if self.curr_reward >= 1:\n",
        "            print(\"Updating!\")\n",
        "            print(state)\n",
        "            print(action)\n",
        "            print(reward)\n",
        "            print(next_state)\n",
        "            print(terminal)\n",
        "            print(td_target)\n",
        "            print(\"\")\n",
        "\n",
        "        self.q_table[state_index, action] = (\n",
        "            1 - self.learning_rate) * self.q_table[state_index, action] + self.learning_rate * td_target"
      ],
      "metadata": {
        "id": "pSK9A3yEXck4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### lstm_dqn"
      ],
      "metadata": {
        "id": "RInPZZ29XShR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LstmDqnAgent(DqnAgent):\n",
        "    \"\"\"Implementation of DQN with an RNN for the q-network.\"\"\"\n",
        "\n",
        "    def __init__(self, sequence_length=0, *args, **kwargs):\n",
        "        self._sequence_length = sequence_length\n",
        "        super(LstmDqnAgent, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _q_network(self, state):\n",
        "        embeddings = tf.get_variable('embeddings',\n",
        "            [self._num_actions + 1, 128])\n",
        "\n",
        "        embedded_ids = tf.gather(embeddings, state)\n",
        "\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(128)\n",
        "\n",
        "        cell_state, hidden_state = tf.nn.dynamic_rnn(\n",
        "            cell=lstm, inputs=embedded_ids, dtype=tf.float32)\n",
        "\n",
        "        q_values = tf.contrib.layers.fully_connected(\n",
        "            hidden_state[1], self._num_actions, activation_fn=None)\n",
        "\n",
        "        return q_values\n",
        "\n",
        "    def _construct_graph(self):\n",
        "        shape=[None]\n",
        "        for dim in self._state_dims:\n",
        "            shape.append(dim)\n",
        "        self._state = tf.placeholder(shape=shape, dtype=tf.int32)\n",
        "\n",
        "        with tf.variable_scope('q_network'):\n",
        "            self._q_values = self._q_network(self._state)\n",
        "        with tf.variable_scope('target_q_network'):\n",
        "            self._target_q_values = self._q_network(self._state)\n",
        "        with tf.variable_scope('q_network_update'):\n",
        "            self._picked_actions = tf.placeholder(shape=[None, 2], dtype=tf.int32)\n",
        "            self._td_targets = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "            self._q_values_pred = tf.gather_nd(self._q_values, self._picked_actions)\n",
        "            self._losses = clipped_error(self._q_values_pred - self._td_targets)\n",
        "            self._loss = tf.reduce_mean(self._losses)\n",
        "\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self._learning_rate)\n",
        "            # self.optimizer = tf.train.RMSPropOptimizer(self._learning_rate, 0.99, 0.0, 1e-6)\n",
        "            # self.optimizer = tf.train.AdamOptimizer(0.0001)\n",
        "            # self.optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
        "\n",
        "            grads_and_vars = self.optimizer.compute_gradients(self._loss, tf.trainable_variables())\n",
        "\n",
        "            grads = [gv[0] for gv in grads_and_vars]\n",
        "            params = [gv[1] for gv in grads_and_vars]\n",
        "\n",
        "            grads = tf.clip_by_global_norm(grads, 5.0)[0]\n",
        "            clipped_grads_and_vars = np.array(list(zip(grads, params)))\n",
        "            self.train_op = self.optimizer.apply_gradients(clipped_grads_and_vars,\n",
        "                global_step=tf.contrib.framework.get_global_step())\n",
        "\n",
        "        with tf.name_scope('target_network_update'):\n",
        "            q_network_params = [t for t in tf.trainable_variables() if t.name.startswith(\n",
        "                'q_network')]\n",
        "            q_network_params = sorted(q_network_params, key=lambda v: v.name)\n",
        "\n",
        "            target_q_network_params = [t for t in tf.trainable_variables() if t.name.startswith(\n",
        "                'target_q_network')]\n",
        "            target_q_network_params = sorted(target_q_network_params, key=lambda v: v.name)\n",
        "\n",
        "            self.target_update_ops = []\n",
        "            for e1_v, e2_v in zip(q_network_params, target_q_network_params):\n",
        "                op = e2_v.assign(e1_v)\n",
        "                self.target_update_ops.append(op)"
      ],
      "metadata": {
        "id": "Sh8bifFeXdOh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### hierarchical_dqn.py"
      ],
      "metadata": {
        "id": "_fSiHfyeXCFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalDqnAgent(object):\n",
        "    INTRINSIC_STEP_COST = -1    # Step cost for the controller.\n",
        "\n",
        "    INTRINSIC_TIME_OUT = 50             # Number of steps after which intrinsic episode ends.\n",
        "    INTRINSIC_TIME_OUT_PENALTY = -10    # Penalty given to controller for timing out episode.\n",
        "\n",
        "    ARTIFICIAL_PENALTY = -100   # Penalty given to the meta-controller for telling the\n",
        "                                # agent to go to the same cluster it is already in.\n",
        "    EXTRA_TRAVEL_PENALTY = -1   # Penalty given to meta-controller if controller agent\n",
        "                                # travels through additional clusters to get to target cluster.\n",
        "    PRETRAIN_EPISODES = 100\n",
        "\n",
        "    def __init__(self,\n",
        "                 learning_rates=[0.1, 0.00025],\n",
        "                 state_sizes=[0, 0],\n",
        "                 agent_types=['network', 'network'],\n",
        "                 subgoals=None,\n",
        "                 num_subgoals=0,\n",
        "                 num_primitive_actions=0,\n",
        "                 meta_controller_state_fn=None,\n",
        "                 check_subgoal_fn=None,\n",
        "                 use_extra_travel_penalty=False,\n",
        "                 use_extra_bit_for_subgoal_center=False,\n",
        "                 use_controller_dqn=False,\n",
        "                 use_intrinsic_timeout=False,\n",
        "                 use_memory=False,\n",
        "                 memory_size=0,\n",
        "                 pretrain_controller=False):\n",
        "        print(\"h-DQN\")\n",
        "        print(\"Use extra travel penalty:\")\n",
        "        print(use_extra_travel_penalty)\n",
        "        print(\"Use extra bit for subgoal center:\")\n",
        "        print(use_extra_bit_for_subgoal_center)\n",
        "        print(\"Use controller dqn:\")\n",
        "        print(use_controller_dqn)\n",
        "        print(\"Use intrinsic timeout:\")\n",
        "        print(use_intrinsic_timeout)\n",
        "        print(\"Use memory:\")\n",
        "        print(use_memory)\n",
        "        print(\"Memory size:\")\n",
        "        print(memory_size)\n",
        "        print(\"Pretrain Controller:\")\n",
        "        print(pretrain_controller)\n",
        "        \"\"\"Initializes a hierarchical DQN agent.\n",
        "           Args:\n",
        "            learning_rates: learning rates of the meta-controller and controller agents.\n",
        "            state_sizes: state sizes of the meta-controller and controller agents.\n",
        "            agent_types: type of each agent - either tabular QLearning agent or Deep Q Network.\n",
        "            subgoals: array of subgoals for the meta-controller.\n",
        "            num_subgoals: the action space of the meta-controller.\n",
        "            num_primitive_actions: the action space of the controller.\n",
        "            meta_controller_state_fn: function that returns the state of the meta-controller.\n",
        "            check_subgoal_fn: function that checks if agent has satisfied a particular subgoal.\n",
        "            use_extra_travel_penalty: whether or not to penalize the meta-controller for bad instructions.\n",
        "            use_extra_bit_for_subgoal_center: whether or not to use an extra bit to indicate whether\n",
        "                                              agent is at center of a particular cluster.\n",
        "            use_controller_dqn: whether to use regular dqn or controller dqn for the controller.\n",
        "            use_intrinsic_timeout: whether or not to intrinsically timeout the controller.\n",
        "        \"\"\"\n",
        "        if not use_extra_travel_penalty:\n",
        "            self.EXTRA_TRAVEL_PENALTY = 0\n",
        "\n",
        "        if use_extra_bit_for_subgoal_center:\n",
        "            self.ARTIFICIAL_PENALTY = 0\n",
        "            state_sizes[0] = state_sizes[0] * 2\n",
        "\n",
        "        if not pretrain_controller:\n",
        "            self.PRETRAIN_EPISODES = 0\n",
        "\n",
        "        if use_memory:\n",
        "            print(\"Decaying meta-controller epsilon faster!\")\n",
        "            self._meta_controller = LstmDqnAgent(num_actions=num_subgoals,\n",
        "                                                 state_dims=[memory_size],\n",
        "                                                 sequence_length=memory_size,\n",
        "                                                 replay_memory_init_size=100,\n",
        "                                                 target_update=100,\n",
        "                                                 epsilon_end=0.01,\n",
        "                                                 epsilon_decay_steps=5000)\n",
        "        else:\n",
        "            self._meta_controller = QLearningAgent(num_states=state_sizes[0],\n",
        "                                                   num_actions=num_subgoals,\n",
        "                                                   learning_rate=learning_rates[0],\n",
        "                                                   epsilon=0.1)\n",
        "        if use_controller_dqn:\n",
        "            self._controller = ControllerDqnAgent(learning_rate=learning_rates[1],\n",
        "                num_actions=num_primitive_actions,\n",
        "                state_dims=state_sizes[1],\n",
        "                subgoal_dims=[num_subgoals])\n",
        "        else:\n",
        "            print(\"Epsilon end for controller is 0.01!\")\n",
        "            self._controller = DqnAgent(learning_rate=learning_rates[1],\n",
        "                num_actions=num_primitive_actions,\n",
        "                state_dims=[state_sizes[1][0] + num_subgoals],\n",
        "                epsilon_end=0.01) # CHANGED\n",
        "\n",
        "        self._subgoals = subgoals\n",
        "        self._num_subgoals = num_subgoals\n",
        "\n",
        "        self._meta_controller_state_fn = meta_controller_state_fn\n",
        "        self._check_subgoal_fn = check_subgoal_fn\n",
        "\n",
        "        self._use_extra_bit_for_subgoal_center = use_extra_bit_for_subgoal_center\n",
        "        self._use_controller_dqn = use_controller_dqn\n",
        "\n",
        "        self._use_intrinsic_timeout = use_intrinsic_timeout\n",
        "\n",
        "        self._use_memory = use_memory\n",
        "        self._memory_size = memory_size\n",
        "\n",
        "\n",
        "        self._meta_controller_state = None\n",
        "        self._curr_subgoal = None\n",
        "        self._meta_controller_reward = 0\n",
        "        self._intermediate_clusters = []\n",
        "        self._intermediate_dict = defaultdict(int)\n",
        "        self._intermediate_clusters_dict = defaultdict(int)\n",
        "        self._history = [0 for i in range(self._memory_size)]\n",
        "\n",
        "        # Only used if use_extra_bit_for_subgoal_center is True.\n",
        "        self._original_state = None\n",
        "\n",
        "        self._next_meta_controller_state = None\n",
        "\n",
        "        self._intrinsic_time_step = 0\n",
        "\n",
        "        self._episode = 0\n",
        "\n",
        "    def update_history(self, state):\n",
        "        returned_state = state\n",
        "        if self._meta_controller_state_fn:\n",
        "            returned_state = self._meta_controller_state_fn(state, self._original_state)\n",
        "\n",
        "        current_cluster_id = np.where(np.squeeze(returned_state) == 1)[0][0] + 1\n",
        "        new_history = self._history[1:]\n",
        "\n",
        "        # print(\"History update!\")\n",
        "        # print(self._history)\n",
        "        # print(new_history)\n",
        "        # print(current_cluster_id)\n",
        "        new_history.append(current_cluster_id)\n",
        "        # print(new_history)\n",
        "        # print(\"\")\n",
        "        self._history = new_history\n",
        "\n",
        "    def get_meta_controller_state(self, state):\n",
        "        returned_state = state\n",
        "        if self._meta_controller_state_fn:\n",
        "            returned_state = self._meta_controller_state_fn(state, self._original_state)\n",
        "\n",
        "        if self._use_memory:\n",
        "            returned_state = self._history[:]\n",
        "\n",
        "        return returned_state\n",
        "\n",
        "    def get_controller_state(self, state, subgoal_index):\n",
        "        curr_subgoal = self._subgoals[subgoal_index]\n",
        "\n",
        "        # Concatenate the environment state with the subgoal.\n",
        "        controller_state = list(state[0])\n",
        "        for i in range(len(curr_subgoal)):\n",
        "            controller_state.append(curr_subgoal[i])\n",
        "        controller_state = np.array([controller_state])\n",
        "        # print(controller_state)\n",
        "        return np.copy(controller_state)\n",
        "\n",
        "    def intrinsic_reward(self, state, subgoal_index):\n",
        "        if self._use_intrinsic_timeout and self._intrinsic_time_step >= self.INTRINSIC_TIME_OUT:\n",
        "            return self.INTRINSIC_TIME_OUT_PENALTY\n",
        "        if self.subgoal_completed(state, subgoal_index):\n",
        "            return 1\n",
        "        else:\n",
        "            return self.INTRINSIC_STEP_COST\n",
        "\n",
        "    def subgoal_completed(self, state, subgoal_index):\n",
        "        if self._check_subgoal_fn is None:\n",
        "            if self._use_intrinsic_timeout and self._intrinsic_time_step >= self.INTRINSIC_TIME_OUT:\n",
        "                return True\n",
        "            return state == self._subgoals[subgoal_index]\n",
        "        else:\n",
        "            if self._use_intrinsic_timeout and self._intrinsic_time_step >= self.INTRINSIC_TIME_OUT:\n",
        "                return True\n",
        "\n",
        "            if not self._use_memory and self._meta_controller_state[self._curr_subgoal] == 1:\n",
        "                if np.sum(self._meta_controller_state) > 1:\n",
        "                    return False\n",
        "\n",
        "                return self._check_subgoal_fn(state, subgoal_index, self._original_state)\n",
        "            else:\n",
        "                return self._check_subgoal_fn(state, subgoal_index)\n",
        "\n",
        "    def store(self, state, action, reward, next_state, terminal, eval=False):\n",
        "        \"\"\"Stores the current transition in replay memory.\n",
        "           The transition is stored in the replay memory of the controller.\n",
        "           If the transition culminates in a subgoal's completion or a terminal state, a\n",
        "           transition for the meta-controller is constructed and stored in its replay buffer.\n",
        "           Args:\n",
        "            state: current state\n",
        "            action: primitive action taken\n",
        "            reward: reward received from state-action pair\n",
        "            next_state: next state\n",
        "            terminal: extrinsic terminal (True or False)\n",
        "            eval: Whether the current episode is a train or eval episode.\n",
        "        \"\"\"\n",
        "\n",
        "        self._meta_controller_reward += reward\n",
        "        self._intrinsic_time_step += 1\n",
        "\n",
        "        # Compute the controller state, reward, next state, and terminal.\n",
        "        intrinsic_state = self.get_controller_state(state, self._curr_subgoal)\n",
        "        intrinsic_next_state = self.get_controller_state(next_state, self._curr_subgoal)\n",
        "        intrinsic_reward = self.intrinsic_reward(next_state, self._curr_subgoal)\n",
        "        subgoal_completed = self.subgoal_completed(next_state, self._curr_subgoal)\n",
        "        intrinsic_terminal = subgoal_completed or terminal\n",
        "\n",
        "        self._controller.store(np.copy(intrinsic_state), action,\n",
        "            intrinsic_reward, np.copy(intrinsic_next_state), intrinsic_terminal, eval)\n",
        "\n",
        "        # Check for intermediate state.\n",
        "        intermediate_meta_controller_state = self.get_meta_controller_state(next_state)\n",
        "\n",
        "        if not self._use_memory:\n",
        "            intermediate_cluster_id = np.where(np.squeeze(intermediate_meta_controller_state) == 1)[0][0]\n",
        "        else:\n",
        "            intermediate_cluster_id = intermediate_meta_controller_state[-1] - 1\n",
        "\n",
        "        self._intermediate_dict[intermediate_cluster_id] += 1\n",
        "        # Agent is traveling through a cluster that is not the starting or ending cluster.\n",
        "        # FIX THIS!!!!\n",
        "        if list(intermediate_meta_controller_state[0:self._num_subgoals]) != list(\n",
        "            self._meta_controller_state[0:self._num_subgoals]) and not subgoal_completed:\n",
        "            self._meta_controller_reward += self.EXTRA_TRAVEL_PENALTY\n",
        "\n",
        "\n",
        "            self._intermediate_clusters.append(intermediate_cluster_id)\n",
        "            self._intermediate_clusters_dict[intermediate_cluster_id] += 1\n",
        "\n",
        "        if terminal and not eval:\n",
        "            self._episode += 1\n",
        "\n",
        "        if subgoal_completed or terminal:\n",
        "            # Normalize the meta-controller reward.\n",
        "            self._meta_controller_reward /= 100.0\n",
        "\n",
        "            meta_controller_state = np.copy(self._meta_controller_state)\n",
        "            if not self._use_memory:\n",
        "                next_meta_controller_state = self.get_meta_controller_state(next_state)\n",
        "            else:\n",
        "                returned_state = self._meta_controller_state_fn(next_state, self._original_state)\n",
        "                current_cluster_id = np.where(np.squeeze(returned_state) == 1)[0][0] + 1\n",
        "                new_history = self._history[1:]\n",
        "                new_history.append(current_cluster_id)\n",
        "                next_meta_controller_state = new_history\n",
        "\n",
        "            if self._episode >= self.PRETRAIN_EPISODES:\n",
        "                self._meta_controller.store(np.copy(meta_controller_state), self._curr_subgoal,\n",
        "                    self._meta_controller_reward, np.copy(next_meta_controller_state),\n",
        "                    terminal, eval, reward)\n",
        "\n",
        "            if eval:\n",
        "                if subgoal_completed:\n",
        "                    print(\"Subgoal completed!\")\n",
        "                    print(\"Intermediate Clusters:\")\n",
        "                    print(self._intermediate_clusters)\n",
        "                    print(\"Intermediate Cluster Count:\")\n",
        "                    print(self._intermediate_dict)\n",
        "                    print(\"Intermediate non-beginning cluster count:\")\n",
        "                    print(self._intermediate_clusters_dict)\n",
        "                    print(\"State:\")\n",
        "                    print(next_state)\n",
        "                    print(\"Meta-Controller reward:\")\n",
        "                    print(self._meta_controller_reward)\n",
        "                    print(\"Intrinsic reward:\")\n",
        "                    print(intrinsic_reward)\n",
        "                    print(\"Cluster:\")\n",
        "                    print(next_meta_controller_state)\n",
        "                    print(\"\")\n",
        "                    print(\"\")\n",
        "                else:\n",
        "                    print(\"Terminal!\")\n",
        "                    print(\"Intermediate clusters:\")\n",
        "                    print(self._intermediate_clusters)\n",
        "                    print(\"Intermediate cluster count:\")\n",
        "                    print(self._intermediate_dict)\n",
        "                    print(\"Intermediate non-beginning cluster count:\")\n",
        "                    print(self._intermediate_clusters_dict)\n",
        "                    print(\"State:\")\n",
        "                    print(next_state)\n",
        "                    print(\"Meta-Controller reward:\")\n",
        "                    print(self._meta_controller_reward)\n",
        "                    print(\"Intrinsic reward:\")\n",
        "                    print(intrinsic_reward)\n",
        "                    print(\"Cluster:\")\n",
        "                    print(next_meta_controller_state)\n",
        "                    print(\"\")\n",
        "                    print(\"\")\n",
        "\n",
        "            # Reset the current meta-controller state and current subgoal to be None\n",
        "            # since the current subgoal is finished. Also reset the meta-controller's reward.\n",
        "            self._next_meta_controller_state = np.copy(next_meta_controller_state)\n",
        "\n",
        "            if terminal:\n",
        "                self._next_meta_controller_state = None\n",
        "\n",
        "            self._meta_controller_state = None\n",
        "            self._curr_subgoal = None\n",
        "            self._meta_controller_reward = 0\n",
        "\n",
        "            self._intermediate_clusters = []\n",
        "            self._intermediate_dict = defaultdict(int)\n",
        "            self._intermediate_clusters_dict = defaultdict(int)\n",
        "\n",
        "            self._original_state = None\n",
        "            self._intrinsic_time_step = 0\n",
        "\n",
        "            if terminal:\n",
        "                self._history = [0 for i in range(self._memory_size)]\n",
        "\n",
        "    def sample(self, state):\n",
        "        \"\"\"Samples an action from the hierarchical DQN agent.\n",
        "           Samples a subgoal if necessary from the meta-controller and samples a primitive action\n",
        "           from the controller.\n",
        "           Args:\n",
        "            state: the current environment state.\n",
        "           Returns:\n",
        "            action: a primitive action.\n",
        "        \"\"\"\n",
        "        if self._meta_controller_state is None:\n",
        "            if self._use_memory:\n",
        "                self.update_history(state)\n",
        "\n",
        "            if self._next_meta_controller_state is not None and not self._use_memory:\n",
        "                self._meta_controller_state = self._next_meta_controller_state\n",
        "            else:\n",
        "                self._meta_controller_state = self.get_meta_controller_state(state)\n",
        "\n",
        "            self._curr_subgoal = self._meta_controller.sample([self._meta_controller_state])\n",
        "\n",
        "            # Artificially penalize the meta-controller for picking the subgoal to\n",
        "            # be the same as the current cluster.\n",
        "            if self._use_memory:\n",
        "                same_cluster_instruction = (self._meta_controller_state[-1] - 1) == self._curr_subgoal\n",
        "            else:\n",
        "                same_cluster_instruction = self._meta_controller_state[self._curr_subgoal] == 1\n",
        "\n",
        "            if same_cluster_instruction:\n",
        "                self._meta_controller_reward = self.ARTIFICIAL_PENALTY\n",
        "                self._original_state = state\n",
        "\n",
        "        controller_state = self.get_controller_state(state, self._curr_subgoal)\n",
        "        action = self._controller.sample(controller_state)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def best_action(self, state):\n",
        "        \"\"\"Returns the greedy action from the hierarchical DQN agent.\n",
        "           Gets the greedy subgoal if necessary from the meta-controller and gets\n",
        "           the greedy primitive action from the controller.\n",
        "           Args:\n",
        "            state: the current environment state.\n",
        "           Returns:\n",
        "            action: the controller's greedy primitive action.\n",
        "        \"\"\"\n",
        "        returned_info = None\n",
        "\n",
        "        if self._meta_controller_state is None:\n",
        "            if self._use_memory:\n",
        "                self.update_history(state)\n",
        "\n",
        "            if self._next_meta_controller_state is not None and not self._use_memory:\n",
        "                self._meta_controller_state = self._next_meta_controller_state\n",
        "            else:\n",
        "                self._meta_controller_state = self.get_meta_controller_state(state)\n",
        "\n",
        "            self._curr_subgoal = self._meta_controller.best_action([self._meta_controller_state])\n",
        "\n",
        "            returned_info = [self._meta_controller_state, self._curr_subgoal]\n",
        "\n",
        "            # Artificially penalize the meta-controller for picking the subgoal to\n",
        "            # be the same as the current cluster.\n",
        "            if self._use_memory:\n",
        "                same_cluster_instruction = (self._meta_controller_state[-1] - 1) == self._curr_subgoal\n",
        "            else:\n",
        "                same_cluster_instruction = self._meta_controller_state[self._curr_subgoal] == 1\n",
        "\n",
        "            if same_cluster_instruction:\n",
        "                self._meta_controller_reward = self.ARTIFICIAL_PENALTY\n",
        "                self._original_state = state\n",
        "\n",
        "            print(\"Current State:\")\n",
        "            print(state)\n",
        "            print(\"Current Meta-Controller State:\")\n",
        "            print(self._meta_controller_state)\n",
        "            print(\"Current subgoal picked:\")\n",
        "            print(self._curr_subgoal)\n",
        "\n",
        "        controller_state = self.get_controller_state(state, self._curr_subgoal)\n",
        "        action = self._controller.best_action(controller_state)\n",
        "        return action, returned_info\n",
        "\n",
        "    def update(self):\n",
        "        self._controller.update()\n",
        "        # Only update meta-controller right after a meta-controller transition has taken place,\n",
        "        # which occurs only when either a subgoal has been completed or the agnent has reached a\n",
        "        # terminal state.\n",
        "        if self._meta_controller_state is None:\n",
        "            self._meta_controller.update()"
      ],
      "metadata": {
        "id": "CPSzzjiWXInc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### controller_dqn.py"
      ],
      "metadata": {
        "id": "efccQ7DUWz_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ControllerDqnAgent(DqnAgent):\n",
        "    def __init__(self, subgoal_dims=[], *args, **kwargs):\n",
        "        self._subgoal_dims = subgoal_dims\n",
        "        super(ControllerDqnAgent, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _q_network(self, state, subgoal):\n",
        "        state_layer1 = tf.contrib.layers.fully_connected(state, 64, activation_fn=tf.nn.relu)\n",
        "        subgoal_layer1 = tf.contrib.layers.fully_connected(subgoal, 64, activation_fn=tf.nn.relu)\n",
        "\n",
        "        layer1 = tf.concat([state_layer1, subgoal_layer1], axis=1)\n",
        "\n",
        "        q_values = tf.contrib.layers.fully_connected(layer1, self._num_actions, activation_fn=None)\n",
        "\n",
        "        return q_values\n",
        "\n",
        "    def _construct_graph(self):\n",
        "        # state_shape=[None]\n",
        "        # subgoal_shape=[None]\n",
        "        # for dim in self._state_dims:\n",
        "        #    state_shape.append(dim)\n",
        "        # for dim in self._subgoal_dims:\n",
        "        #    subgoal_shape.append(dim)\n",
        "        state_shape = self._state_dims[0]\n",
        "        subgoal_shape = self._subgoal_dims[0]\n",
        "\n",
        "        self._state = tf.placeholder(shape=[None, state_shape + subgoal_shape],\n",
        "            dtype=tf.float32)\n",
        "        self._controller_state, self._subgoal = tf.split(\n",
        "            self._state, [state_shape, subgoal_shape], axis=1)\n",
        "\n",
        "        with tf.variable_scope('q_network'):\n",
        "            self._q_values = self._q_network(self._controller_state, self._subgoal)\n",
        "        with tf.variable_scope('target_q_network'):\n",
        "            self._target_q_values = self._q_network(self._controller_state, self._subgoal)\n",
        "        with tf.variable_scope('q_network_update'):\n",
        "            self._picked_actions = tf.placeholder(shape=[None, 2], dtype=tf.int32)\n",
        "            self._td_targets = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "\n",
        "            self._q_values_pred = tf.gather_nd(self._q_values, self._picked_actions)\n",
        "\n",
        "            self._losses = clipped_error(self._q_values_pred - self._td_targets)\n",
        "            self._loss = tf.reduce_mean(self._losses)\n",
        "\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self._learning_rate)\n",
        "            # self.optimizer = tf.train.RMSPropOptimizer(self._learning_rate, 0.99, 0.0, 1e-6)\n",
        "            # self.optimizer = tf.train.AdamOptimizer(0.0001)\n",
        "            # self.optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
        "\n",
        "            grads_and_vars = self.optimizer.compute_gradients(self._loss, tf.trainable_variables())\n",
        "\n",
        "            grads = [gv[0] for gv in grads_and_vars]\n",
        "            params = [gv[1] for gv in grads_and_vars]\n",
        "\n",
        "            grads = tf.clip_by_global_norm(grads, 5.0)[0]\n",
        "\n",
        "            # clipped_grads_and_vars = [(\n",
        "            #    tf.clip_by_norm(grad, 5.0), var) for grad, var in grads_and_vars]\n",
        "            clipped_grads_and_vars = np.array(list(zip(grads, params)))\n",
        "            self.train_op = self.optimizer.apply_gradients(clipped_grads_and_vars,\n",
        "                global_step=tf.contrib.framework.get_global_step())\n",
        "\n",
        "            # self.train_op = self.optimizer.minimize(self._loss,\n",
        "            #    global_step=tf.contrib.framework.get_global_step())\n",
        "        with tf.name_scope('target_network_update'):\n",
        "            q_network_params = [t for t in tf.trainable_variables() if t.name.startswith(\n",
        "                'q_network')]\n",
        "            q_network_params = sorted(q_network_params, key=lambda v: v.name)\n",
        "\n",
        "            target_q_network_params = [t for t in tf.trainable_variables() if t.name.startswith(\n",
        "                'target_q_network')]\n",
        "            target_q_network_params = sorted(target_q_network_params, key=lambda v: v.name)\n",
        "\n",
        "            self.target_update_ops = []\n",
        "            for e1_v, e2_v in zip(q_network_params, target_q_network_params):\n",
        "                op = e2_v.assign(e1_v)\n",
        "                self.target_update_ops.append(op)"
      ],
      "metadata": {
        "id": "ueowqPV7WnoK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functional"
      ],
      "metadata": {
        "id": "poG6UQ3oQ1So"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### clustering.py"
      ],
      "metadata": {
        "id": "DLe1QmlaW_gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_clusters(env_name, n_clusters):\n",
        "    env = gym.make(env_name)\n",
        "    env.reset()\n",
        "    VALID_ACTIONS = list(range(env.action_space.n))\n",
        "    data = []\n",
        "\n",
        "    for episode in range(1000):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        step_count = 0\n",
        "        while not done:\n",
        "            step_count += 1\n",
        "            action = random.randint(0, len(VALID_ACTIONS) - 1)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            state = next_state\n",
        "            state_with_reward = state[:]\n",
        "            state_with_reward = np.append(state_with_reward, reward)\n",
        "            data.append(state_with_reward)\n",
        "\n",
        "    data = np.array(data)\n",
        "    x_pos_normalized = (data[:, 0] - np.mean(data[:, 0])) / np.std(data[:, 0])\n",
        "    velocity_normalized = (data[:, 1] - np.mean(data[:, 1])) / np.std(data[:, 1])\n",
        "    # reward_normalized = (data[:, 2] - np.mean(data[:, 2])) / np.std(data[:, 2])\n",
        "    # data_normalized = zip(x_pos_normalized, velocity_normalized, reward_normalized)\n",
        "    data_normalized = np.array(list(zip(x_pos_normalized, velocity_normalized)))\n",
        "    # print(data_normalized)\n",
        "    estimator = KMeans(n_clusters=n_clusters)\n",
        "    estimator.fit(data_normalized)\n",
        "    means = [np.mean(data[:, 0]), np.mean(data[:, 1])]\n",
        "    stds = [np.std(data[:, 0]), np.std(data[:, 1])]\n",
        "    cluster_centers = estimator.cluster_centers_[:,0:2]\n",
        "    for i in range(len(cluster_centers)):\n",
        "        cluster_centers[i][0] = cluster_centers[i][0] * stds[0] + means[0]\n",
        "        cluster_centers[i][1] = cluster_centers[i][1] * stds[1] + means[1]\n",
        "    labels = estimator.labels_\n",
        "    labels = labels.astype(np.int32)\n",
        "    colors = ['red', 'green', 'blue', 'orange',\n",
        "    'yellow', 'magenta', 'black',\n",
        "    'purple', 'brown', 'white']\n",
        "    fig, ax = plt.subplots()\n",
        "    for i in range(n_clusters):\n",
        "        label = i\n",
        "        color = colors[i%len(colors)]\n",
        "        indices_of_labels = np.where(labels==label)\n",
        "        ax.scatter(data[indices_of_labels,0][0], data[indices_of_labels,1][0], c=color,\n",
        "            label=int(label), alpha=0.5)\n",
        "    ax.legend()\n",
        "    plt.xlabel('X Position')\n",
        "    plt.ylabel('Velocity')\n",
        "    try:\n",
        "        os.stat('clusters_' + str(n_clusters))\n",
        "    except:\n",
        "        os.mkdir('clusters_' + str(n_clusters))\n",
        "\n",
        "    plt.savefig('clusters_' + str(n_clusters) + '/Clusters.png')\n",
        "    print('9')\n",
        "\n",
        "    returned_data = np.array(list(zip(data[:, 0], data[:, 1])))\n",
        "\n",
        "    with open('clusters_' + str(n_clusters) + '/data', 'wb') as data_file:\n",
        "        pickle.dump(returned_data, data_file)\n",
        "    with open('clusters_' + str(n_clusters) + '/labels', 'wb') as labels_file:\n",
        "        pickle.dump(labels, labels_file)\n",
        "    with open('clusters_' + str(n_clusters) + '/cluster_centers', 'wb') as cluster_centers_file:\n",
        "        pickle.dump(cluster_centers, cluster_centers_file)\n",
        "    print('10')\n",
        "\n",
        "    return returned_data, labels, cluster_centers\n",
        "\n",
        "\n",
        "def get_cluster_fn(env_name='MountainCar-v0', n_clusters=10, extra_bit=True, load_from_dir=True):\n",
        "    if load_from_dir:\n",
        "        with open('clusters_' + str(n_clusters) + '/data', 'rb') as data_file:\n",
        "            data = pickle.load(data_file)\n",
        "        with open('clusters_' + str(n_clusters) + '/labels', 'rb') as labels_file:\n",
        "            labels = pickle.load(labels_file)\n",
        "        with open('clusters_' + str(n_clusters) + '/cluster_centers', 'rb') as cluster_centers_file:\n",
        "            cluster_centers = pickle.load(cluster_centers_file)\n",
        "\n",
        "    else:\n",
        "        data, labels, cluster_centers = make_clusters(env_name, n_clusters)\n",
        "        print('xxx')\n",
        "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "    neigh.fit(data, labels)\n",
        "    # Create one-hot representation of the clusters.\n",
        "    clusters_one_hot = [np.zeros(n_clusters) for i in range(n_clusters)]\n",
        "    for i in range(len(clusters_one_hot)):\n",
        "        clusters_one_hot[i][i] = 1\n",
        "\n",
        "    ratio = 0.5\n",
        "\n",
        "    def check_cluster(data_point, cluster_index, original_point=None):\n",
        "        # print(\"Check cluster function:\")\n",
        "        # print(cluster_index)\n",
        "        if not extra_bit or original_point is None:\n",
        "            predicted_cluster_index = neigh.predict(data_point)[0]\n",
        "            data_point = np.squeeze(data_point)\n",
        "            # Cheating for the goal cluster area!\n",
        "            if data_point[0] >= 0.5:\n",
        "                predicted_cluster_index = 5\n",
        "            if data_point[0] < 0.5 and predicted_cluster_index == 5:\n",
        "                predicted_cluster_index = 3 \n",
        "            return cluster_index == predicted_cluster_index\n",
        "        else:\n",
        "            distance_to_boundary = euclidean_distance(data_point, original_point)\n",
        "            distance_to_center = euclidean_distance(data_point, cluster_centers[cluster_index])\n",
        "            return np.float(distance_to_center) / np.maximum(distance_to_boundary, np.exp(-10)) <= ratio\n",
        "\n",
        "\n",
        "    def identify_cluster(data_point, original_point):\n",
        "        cluster_index = neigh.predict(data_point)[0]\n",
        "        data_point = np.squeeze(data_point)\n",
        "        # Cheating for the goal cluster area!\n",
        "        if data_point[0] >= 0.5:\n",
        "            cluster_index = 5\n",
        "        if data_point[0] < 0.5 and cluster_index == 5:\n",
        "            cluster_index = 3\n",
        "\n",
        "        if extra_bit:\n",
        "            cluster_one_hot = np.zeros(n_clusters + 1)\n",
        "        else:\n",
        "            cluster_one_hot = np.zeros(n_clusters)\n",
        "        cluster_one_hot[cluster_index] = 1\n",
        "\n",
        "        if extra_bit:\n",
        "            # Add bit that represents whether agent is on boundary or in center of cluster\n",
        "            if original_point is not None:\n",
        "                distance_to_boundary = euclidean_distance(data_point, original_point)\n",
        "                distance_to_center = euclidean_distance(data_point, cluster_centers[cluster_index])\n",
        "                if np.float(distance_to_center) / np.maximum(distance_to_boundary, np.exp(-10)) <= ratio:\n",
        "                    cluster_one_hot[-1] = 1\n",
        "\n",
        "        return cluster_one_hot\n",
        "\n",
        "    return identify_cluster, check_cluster, n_clusters, np.array(clusters_one_hot)\n",
        "\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    point1 = np.squeeze(point1)\n",
        "    point2 = np.squeeze(point2)\n",
        "    return np.sqrt(np.square(point1[0] - point2[0]) + np.square(point1[1] - point2[1]))"
      ],
      "metadata": {
        "id": "uc1d221sXBv2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train.py"
      ],
      "metadata": {
        "id": "h3Fn-yTYW4FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.flags.DEFINE_string('agent_type', 'h_dqn', 'RL agent type.')\n",
        "# tf.flags.DEFINE_integer('n_clusters', 6, 'Number of clusters to form in unsupervised training.')\n",
        "# tf.flags.DEFINE_string('logdir', 'experiment_logs/Cheating_Epsilon_Decay_Faster/', 'Directory of logfile.')\n",
        "# tf.flags.DEFINE_string('experiment_dir', '', 'Directory of experiment files.')\n",
        "# tf.flags.DEFINE_string('logfile', 'log.txt', 'Name of the logfile.')\n",
        "# tf.flags.DEFINE_string('env_name', 'MountainCar-v0', 'Name of the environment.')\n",
        "# tf.flags.DEFINE_boolean('use_extra_travel_penalty', False, 'Whether or not to penalize meta-controller for sending agent to non-adjacent clusters.')\n",
        "# tf.flags.DEFINE_boolean('use_extra_bit', False, 'Whether or not the meta-controller state contains an extra bit which indicates whether or not the agent is near the center of a particular cluster.')\n",
        "# tf.flags.DEFINE_boolean('use_controller_dqn', False, 'Whether to use a controller dqn as opposed to normal dqn for the controller.')\n",
        "# tf.flags.DEFINE_boolean('use_intrinsic_timeout', False, 'Whether or not to intrinsically timeout controller agent.')\n",
        "# tf.flags.DEFINE_boolean('use_memory', False, 'Whether or not the meta-controller should use memory.')\n",
        "# tf.flags.DEFINE_integer('memory_size', 5, 'Size of the LSTM memory.')\n",
        "# tf.flags.DEFINE_boolean('pretrain_controller', False, 'Whether or not to pretrain the controller.')\n",
        "# tf.flags.DEFINE_integer('run_number', 1, 'Run number.')\n",
        "\n",
        "agent_type = \"h_dqn\"\n",
        "n_clusters = 6\n",
        "logdir = 'experiment_logs/Cheating_Epsilon_Decay_Faster/'\n",
        "experiment_dir = \"\"\n",
        "logfile = \"log.txt\"\n",
        "env_name = 'MountainCar-v0'\n",
        "use_extra_travel_penalty = False\n",
        "use_extra_bit = False\n",
        "use_controller_dqn = False\n",
        "use_intrinsic_timeout = False\n",
        "use_memory = False\n",
        "memory_size = 5\n",
        "pretrain_controller = False\n",
        "run_number = 1\n",
        "\n",
        "env_name = ''\n",
        "\n",
        "# FLAGS = tf.flags.FLAGS\n",
        "\n",
        "def log(logfile, iteration, rewards):\n",
        "    \"\"\"Function that logs the reward statistics obtained by the agent.\n",
        "    Args:\n",
        "        logfile: File to log reward statistics.\n",
        "        iteration: The current iteration.\n",
        "        rewards: Array of rewards obtained in the current iteration.\n",
        "    \"\"\"\n",
        "    log_string = '{} {} {} {}'.format(\n",
        "        iteration, np.min(rewards), np.mean(rewards), np.max(rewards))\n",
        "    print(log_string)\n",
        "\n",
        "    with open(logfile, 'a') as f:\n",
        "        f.write(log_string + '\\n')\n",
        "\n",
        "def make_environment(env_name):\n",
        "    return gym.make(env_name)\n",
        "\n",
        "def make_agent(agent_type, env, num_clusters, use_extra_travel_penalty, use_extra_bit,\n",
        "    use_controller_dqn, use_intrinsic_timeout, use_memory, memory_size, pretrain_controller):\n",
        "    if agent_type == 'dqn':\n",
        "        return DqnAgent(state_dims=[2],\n",
        "                            num_actions=2) # env.action_space.n\n",
        "    elif agent_type == 'h_dqn':\n",
        "        meta_controller_state_fn, check_subgoal_fn, num_subgoals, subgoals = get_cluster_fn(\n",
        "                                                                                            n_clusters=num_clusters, \n",
        "                                                                                            extra_bit=use_extra_bit, \n",
        "                                                                                            load_from_dir=False\n",
        "                                                                                            )\n",
        "\n",
        "        return HierarchicalDqnAgent(\n",
        "            state_sizes=[num_subgoals, [2]],\n",
        "            agent_types=['tabular', 'network'],\n",
        "            subgoals=subgoals,\n",
        "            num_subgoals=num_subgoals,\n",
        "            num_primitive_actions=2, # env.action_space.n\n",
        "            meta_controller_state_fn=meta_controller_state_fn,\n",
        "            check_subgoal_fn=check_subgoal_fn,\n",
        "            use_extra_travel_penalty=use_extra_travel_penalty,\n",
        "            use_extra_bit_for_subgoal_center=use_extra_bit,\n",
        "            use_controller_dqn=use_controller_dqn,\n",
        "            use_intrinsic_timeout=use_intrinsic_timeout,\n",
        "            use_memory=use_memory,\n",
        "            memory_size=memory_size,\n",
        "            pretrain_controller=pretrain_controller)\n",
        "\n",
        "def run(env_name='MountainCar-v0',\n",
        "        agent_type='dqn',\n",
        "        num_iterations=10000000,\n",
        "        num_train_episodes=100,\n",
        "        num_eval_episodes=100,\n",
        "        num_clusters=5,\n",
        "        logdir=None,\n",
        "        experiment_dir=None,\n",
        "        logfile=None,\n",
        "        use_extra_travel_penalty=False,\n",
        "        use_extra_bit=False,\n",
        "        use_controller_dqn=False,\n",
        "        use_intrinsic_timeout=False,\n",
        "        use_memory=False,\n",
        "        memory_size=5,\n",
        "        pretrain_controller=False,\n",
        "        run_number=1):\n",
        "    \"\"\"Function that executes RL training and evaluation.\n",
        "    Args:\n",
        "        env_name: Name of the environment that the agent will interact with.\n",
        "        agent_type: The type RL agent that will be used for training.\n",
        "        num_iterations: Number of iterations to train for.\n",
        "        num_train_episodes: Number of training episodes per iteration.\n",
        "        num_eval_episodes: Number of evaluation episodes per iteration.\n",
        "        num_clusters: The number of clusters to use for the h-DQN unsupervised clustering.\n",
        "        logdir: Directory for log file.\n",
        "        logfile: File to log the agent's performance over training.\n",
        "    \"\"\"\n",
        "    print(agent_type)\n",
        "    print(num_clusters)\n",
        "    print(use_extra_bit)\n",
        "    experiment_dir += '_agent_type_' + agent_type + '_num_clusters_' + str(\n",
        "        num_clusters) + '_use_extra_travel_penalty_' + str(\n",
        "        use_extra_travel_penalty) + '_use_extra_bit_' + str(\n",
        "        use_extra_bit) + '_use_controller_dqn_' + str(\n",
        "        use_controller_dqn) + '_use_intrinsic_timeout_' + str(\n",
        "        use_intrinsic_timeout) + '_use_memory_' + str(\n",
        "        use_memory) + '_memory_size_' + str(\n",
        "        memory_size) + '_pretrain_controller_' + str(\n",
        "        pretrain_controller) + '_run_number_' + str(run_number)\n",
        "\n",
        "    experiment_dir = logdir + experiment_dir\n",
        "    logfile = experiment_dir + '/' + logfile\n",
        "\n",
        "    try:\n",
        "        os.stat(experiment_dir)\n",
        "    except:\n",
        "        os.mkdir(experiment_dir)\n",
        "\n",
        "    env = make_environment(env_name)\n",
        "    env_test = make_environment(env_name)\n",
        "    # env_test = Monitor(env_test, directory='videos/', video_callable=lambda x: True, resume=True)\n",
        "    print('Made environment!')\n",
        "    agent = make_agent(agent_type, env, num_clusters, use_extra_travel_penalty, use_extra_bit,\n",
        "        use_controller_dqn, use_intrinsic_timeout, use_memory, memory_size, pretrain_controller)\n",
        "    print('Made agent!')\n",
        "\n",
        "    for it in range(num_iterations):\n",
        "        # Run train episodes.\n",
        "        for train_episode in range(num_train_episodes):\n",
        "            # Reset the environment.\n",
        "            state = env.reset()\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "\n",
        "            episode_reward = 0\n",
        "\n",
        "            # Run the episode.\n",
        "            terminal = False\n",
        "\n",
        "            while not terminal:\n",
        "                action = agent.sample(state)\n",
        "                # Remove the do-nothing action.\n",
        "                if action == 1:\n",
        "                    env_action = 2\n",
        "                else:\n",
        "                    env_action = action\n",
        "\n",
        "                next_state, reward, terminal, _ = env.step(env_action)\n",
        "                next_state = np.expand_dims(next_state, axis=0)\n",
        "\n",
        "                agent.store(state, action, reward, next_state, terminal)\n",
        "                agent.update()\n",
        "\n",
        "                episode_reward += reward\n",
        "                # Update the state.\n",
        "                state = next_state\n",
        "\n",
        "        eval_rewards = []\n",
        "\n",
        "        heat_map = np.zeros((num_clusters, num_clusters))\n",
        "\n",
        "        # Run eval episodes.\n",
        "        for eval_episode in range(num_eval_episodes):\n",
        "\n",
        "            # Reset the environment.\n",
        "            state = env_test.reset()\n",
        "            # env_test.render()\n",
        "\n",
        "            # Make sure that at test time, the agent starts near bottom of the hill.\n",
        "            while state[0] < -0.6 or state[0] > -0.4:\n",
        "                state = env_test.reset()\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "\n",
        "            episode_reward = 0\n",
        "\n",
        "            # Run the episode.\n",
        "            terminal = False\n",
        "\n",
        "            while not terminal:\n",
        "                if agent_type == 'dqn':\n",
        "                    action = agent.best_action(state)\n",
        "                else:\n",
        "                    action, info = agent.best_action(state)\n",
        "                if agent_type == 'h_dqn' and info is not None:\n",
        "                    curr_state = info[0]\n",
        "                    if not use_memory:\n",
        "                        curr_state = np.where(np.squeeze(curr_state) == 1)[0][0]\n",
        "                    else:\n",
        "                        curr_state = np.squeeze(curr_state)[-1] - 1\n",
        "                    goal = info[1]\n",
        "                    heat_map[curr_state][goal] += 1\n",
        "\n",
        "                # Remove the do-nothing action.\n",
        "                if action == 1:\n",
        "                    env_action = 2\n",
        "                else:\n",
        "                    env_action = action\n",
        "\n",
        "                next_state, reward, terminal, _ = env_test.step(env_action)\n",
        "\n",
        "                next_state = np.expand_dims(next_state, axis=0)\n",
        "                # env_test.render()\n",
        "                agent.store(state, action, reward, next_state, terminal, eval=True)\n",
        "                if reward > 1:\n",
        "                    reward = 1 # For sake of comparison.\n",
        "\n",
        "                episode_reward += reward\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            eval_rewards.append(episode_reward)\n",
        "\n",
        "        with open(experiment_dir + '/eval_rewards_' + str(it), 'wb') as f:\n",
        "            pickle.dump(eval_rewards, f)\n",
        "\n",
        "        log(logfile, it, eval_rewards)\n",
        "        if agent_type == 'h_dqn':\n",
        "                plt.figure()\n",
        "                plt.imshow(heat_map, cmap='hot', interpolation='nearest')\n",
        "                plt.savefig(experiment_dir + '/heatmap_' + str(it) + '.png')\n",
        "\n",
        "run(agent_type=agent_type, logdir=logdir, experiment_dir=experiment_dir,\n",
        "    logfile=logfile, num_clusters=n_clusters,\n",
        "    use_extra_travel_penalty=use_extra_travel_penalty, use_extra_bit=use_extra_bit,\n",
        "    use_controller_dqn=use_controller_dqn, use_intrinsic_timeout=use_intrinsic_timeout,\n",
        "    use_memory=use_memory, memory_size=memory_size,\n",
        "    pretrain_controller=pretrain_controller, run_number=run_number)\n"
      ],
      "metadata": {
        "id": "ThXTb_fSWOjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d43127-9fa3-41fb-d852-bc8f124bf544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.48607906  0.00857885]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.47678101  0.00929805]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.47678101  0.00929805]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46683294  0.00994807]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46683294  0.00994807]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45630856  0.01052438]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45630856  0.01052438]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.44528544  0.01102312]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44528544  0.01102312]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.43384428  0.01144116]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.43384428  0.01144116]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.42206817  0.01177611]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.42206817  0.01177611]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41004182  0.01202635]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41004182  0.01202635]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.39785077  0.01219105]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.39785077  0.01219105]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38558062  0.01227015]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38558062  0.01227015]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.37331629  0.01226434]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.37331629  0.01226434]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36114127  0.01217502]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36114127  0.01217502]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34913701  0.01200425]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34913701  0.01200425]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.3373823   0.01175472]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.3373823   0.01175472]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32595269  0.01142961]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32595269  0.01142961]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31492008  0.01103261]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31492008  0.01103261]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30435231  0.01056777]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30435231  0.01056777]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.29431286  0.01003944]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.29431286  0.01003944]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.28486063  0.00945224]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.28486063  0.00945224]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.27604973  0.0088109 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.27604973  0.0088109 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.26792943  0.0081203 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.26792943  0.0081203 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.26054409  0.00738534]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.26054409  0.00738534]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.25393316  0.00661093]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.25393316  0.00661093]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.24813122  0.00580194]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.24813122  0.00580194]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.24316803  0.00496319]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.24316803  0.00496319]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.23906859  0.00409943]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.23906859  0.00409943]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.23585328  0.00321532]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.23585328  0.00321532]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {2: 10, 1: 13, 0: 12, 4: 12, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 10, 1: 13, 0: 12, 4: 12})\n",
            "State:\n",
            "[[-0.46557684  0.03888287]]\n",
            "Meta-Controller reward:\n",
            "-1.48\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46557684  0.03888287]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.42612694  0.0394499 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.42612694  0.0394499 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.3863977   0.03972924]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.3863977   0.03972924]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34666866  0.03972904]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34666866  0.03972904]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30720516  0.0394635 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30720516  0.0394635 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.26825299  0.03895217]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.26825299  0.03895217]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.23003404  0.03821895]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.23003404  0.03821895]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.19274304  0.037291  ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.19274304  0.037291  ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.15654562  0.03619742]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.15654562  0.03619742]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.12157753  0.03496809]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.12157753  0.03496809]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.08794498  0.03363254]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.08794498  0.03363254]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {2: 27, 1: 8, 0: 23, 4: 7, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 27, 1: 8, 0: 23, 4: 7})\n",
            "State:\n",
            "[[-0.44911739  0.06191862]]\n",
            "Meta-Controller reward:\n",
            "-1.66\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44911739  0.06191862]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38675274  0.06236465]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38675274  0.06236465]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32438585  0.06236689]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32438585  0.06236689]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.26242569  0.06196016]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.26242569  0.06196016]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.20122997  0.06119572]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.20122997  0.06119572]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.14109236  0.0601376 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.14109236  0.0601376 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.08223413  0.05885823]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.08223413  0.05885823]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Terminal!\n",
            "Intermediate clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5]\n",
            "Intermediate cluster count:\n",
            "defaultdict(<class 'int'>, {2: 12, 5: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 12, 5: 1})\n",
            "State:\n",
            "[[0.51299153 0.0338328 ]]\n",
            "Meta-Controller reward:\n",
            "-1.13\n",
            "Intrinsic reward:\n",
            "-1\n",
            "Cluster:\n",
            "[0. 0. 0. 0. 0. 1.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.4703569  0.       ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46975451  0.00060238]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46975451  0.00060238]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46855421  0.0012003 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46855421  0.0012003 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46676487  0.00178934]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46676487  0.00178934]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46439972  0.00236515]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46439972  0.00236515]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46147623  0.00292349]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46147623  0.00292349]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45801597  0.00346026]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45801597  0.00346026]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45404441  0.00397156]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45404441  0.00397156]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.44959073  0.00445368]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44959073  0.00445368]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.44468757  0.00490317]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44468757  0.00490317]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.43937072  0.00531684]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.43937072  0.00531684]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.43367889  0.00569184]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.43367889  0.00569184]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.4276533   0.00602559]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.4276533   0.00602559]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.42133741  0.00631589]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.42133741  0.00631589]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41477651  0.0065609 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41477651  0.0065609 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.40801735  0.00675916]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.40801735  0.00675916]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.40110779  0.00690956]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.40110779  0.00690956]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.39409637  0.00701142]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.39409637  0.00701142]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38703197  0.0070644 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38703197  0.0070644 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.37996341  0.00706856]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.37996341  0.00706856]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.37293908  0.00702433]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.37293908  0.00702433]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36600662  0.00693246]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36600662  0.00693246]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.35921256  0.00679406]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.35921256  0.00679406]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.35260202  0.00661053]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.35260202  0.00661053]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34621845  0.00638357]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34621845  0.00638357]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34010333  0.00611512]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34010333  0.00611512]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.33429596  0.00580737]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.33429596  0.00580737]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32883326  0.00546269]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32883326  0.00546269]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.3237496   0.00508366]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.3237496   0.00508366]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31907661  0.00467299]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31907661  0.00467299]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31484309  0.00423352]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31484309  0.00423352]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31107488  0.00376821]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31107488  0.00376821]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30779478  0.0032801 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30779478  0.0032801 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30502249  0.00277229]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30502249  0.00277229]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30277455  0.00224794]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30277455  0.00224794]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {2: 41, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 41})\n",
            "State:\n",
            "[[-4.70401021e-01 -1.07345338e-04]]\n",
            "Meta-Controller reward:\n",
            "-1.42\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-4.70401021e-01 -1.07345338e-04]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46990566  0.00049536]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46990566  0.00049536]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46881125  0.0010944 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46881125  0.0010944 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46712591  0.00168535]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46712591  0.00168535]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46486209  0.00226382]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46486209  0.00226382]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46203651  0.00282557]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46203651  0.00282557]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45867004  0.00336648]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45867004  0.00336648]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45478745  0.00388258]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45478745  0.00388258]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.4504173   0.00437016]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.4504173   0.00437016]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.4455916   0.00482569]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.4455916   0.00482569]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.44034564  0.00524597]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44034564  0.00524597]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.4347176   0.00562804]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.4347176   0.00562804]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.4287483  0.0059693]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.4287483  0.0059693]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.42248081  0.00626748]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.42248081  0.00626748]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41596013  0.00652068]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41596013  0.00652068]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.40923279  0.00672734]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.40923279  0.00672734]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.40234646  0.00688633]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.40234646  0.00688633]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.3953496   0.00699686]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.3953496   0.00699686]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38829105  0.00705855]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38829105  0.00705855]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38121967  0.00707138]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38121967  0.00707138]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.37418396  0.00703571]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.37418396  0.00703571]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36723171  0.00695225]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36723171  0.00695225]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36040967  0.00682204]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36040967  0.00682204]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.35376324  0.00664643]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.35376324  0.00664643]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34733618  0.00642706]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34733618  0.00642706]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34117033  0.00616585]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34117033  0.00616585]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.33530542  0.00586492]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.33530542  0.00586492]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32977878  0.00552663]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32977878  0.00552663]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32462526  0.00515352]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32462526  0.00515352]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31987699  0.00474827]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31987699  0.00474827]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31556327  0.00431372]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31556327  0.00431372]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31171048  0.00385279]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31171048  0.00385279]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30834198  0.00336851]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30834198  0.00336851]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.305478    0.00286397]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.305478    0.00286397]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {2: 42, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 42})\n",
            "State:\n",
            "[[-4.70426041e-01 -2.14665478e-04]]\n",
            "Meta-Controller reward:\n",
            "-1.43\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-4.70426041e-01 -2.14665478e-04]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-4.70037813e-01  3.88228160e-04]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-4.70037813e-01  3.88228160e-04]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46904957  0.00098825]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46904957  0.00098825]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46746861  0.00158095]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46746861  0.00158095]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46530665  0.00216196]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46530665  0.00216196]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46257966  0.002727  ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46257966  0.002727  ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45930775  0.0032719 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45930775  0.0032719 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45551504  0.00379271]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45551504  0.00379271]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45122942  0.00428562]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45122942  0.00428562]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.44648232  0.0047471 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44648232  0.0047471 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.44130844  0.00517388]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44130844  0.00517388]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.43574549  0.00556295]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.43574549  0.00556295]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.42983384  0.00591165]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.42983384  0.00591165]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.42361619  0.00621765]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.42361619  0.00621765]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41713721  0.00647898]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41713721  0.00647898]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41044319  0.00669402]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41044319  0.00669402]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.40358163  0.00686156]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.40358163  0.00686156]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.39660088  0.00698075]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.39660088  0.00698075]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38954973  0.00705115]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38954973  0.00705115]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38247708  0.00707266]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38247708  0.00707266]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.37543149  0.00704558]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.37543149  0.00704558]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36846093  0.00697056]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36846093  0.00697056]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36161235  0.00684858]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36161235  0.00684858]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.35493142  0.00668094]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.35493142  0.00668094]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34846219  0.00646923]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34846219  0.00646923]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.34224688  0.00621531]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.34224688  0.00621531]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.3363256   0.00592128]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.3363256   0.00592128]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.33073613  0.00558947]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.33073613  0.00558947]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32551377  0.00522236]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32551377  0.00522236]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32069114  0.00482263]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32069114  0.00482263]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31629806  0.00439308]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31629806  0.00439308]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31236144  0.00393662]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31236144  0.00393662]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30890516  0.00345627]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30890516  0.00345627]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30595005  0.00295512]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30595005  0.00295512]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Terminal!\n",
            "Intermediate clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Intermediate cluster count:\n",
            "defaultdict(<class 'int'>, {2: 15})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 15})\n",
            "State:\n",
            "[[-0.32606952 -0.00486814]]\n",
            "Meta-Controller reward:\n",
            "-1.15\n",
            "Intrinsic reward:\n",
            "-1\n",
            "Cluster:\n",
            "[0. 0. 1. 0. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.57727885  0.        ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.57587798  0.00140086]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.57587798  0.00140086]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.57308663  0.00279135]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.57308663  0.00279135]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[4, 4, 4, 4, 4, 4, 4, 4]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {4: 8, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {4: 8})\n",
            "State:\n",
            "[[-0.49205277  0.01320874]]\n",
            "Meta-Controller reward:\n",
            "-1.09\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.49205277  0.01320874]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.47808027  0.0139725 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.47808027  0.0139725 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46344809  0.01463217]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46344809  0.01463217]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.4482646   0.01518349]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.4482646   0.01518349]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.43264132  0.01562328]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.43264132  0.01562328]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41669179  0.01594953]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41669179  0.01594953]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.40053039  0.01616141]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.40053039  0.01616141]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38427117  0.01625922]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38427117  0.01625922]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36802674  0.01624443]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36802674  0.01624443]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.3519072   0.01611954]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.3519072   0.01611954]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.33601917  0.01588804]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.33601917  0.01588804]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32046489  0.01555427]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32046489  0.01555427]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30534156  0.01512333]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30534156  0.01512333]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.29074068  0.01460089]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.29074068  0.01460089]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.27674761  0.01399307]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.27674761  0.01399307]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.26344128  0.01330633]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.26344128  0.01330633]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.25089399  0.01254729]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.25089399  0.01254729]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.23917134  0.01172265]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.23917134  0.01172265]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.2283323   0.01083904]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.2283323   0.01083904]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.21842932  0.00990299]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.21842932  0.00990299]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.20950851  0.00892081]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.20950851  0.00892081]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.20160994  0.00789857]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.20160994  0.00789857]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[2]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {2: 1, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 1})\n",
            "State:\n",
            "[[-0.19101105  0.00375683]]\n",
            "Meta-Controller reward:\n",
            "-1.02\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.19101105  0.00375683]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {2: 11, 1: 12, 0: 15, 4: 11, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 11, 1: 12, 0: 15, 4: 11})\n",
            "State:\n",
            "[[-0.46052976  0.0432293 ]]\n",
            "Meta-Controller reward:\n",
            "-1.5\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46052976  0.0432293 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41677066  0.0437591 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41677066  0.0437591 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.37279912  0.04397153]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.37279912  0.04397153]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.3289204   0.04387872]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.3289204   0.04387872]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.28542017  0.04350024]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.28542017  0.04350024]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.2425581   0.04286207]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.2425581   0.04286207]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.20056284  0.04199526]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.20056284  0.04199526]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.15962853  0.04093431]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.15962853  0.04093431]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.119913    0.03971553]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.119913    0.03971553]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.08153743  0.03837556]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.08153743  0.03837556]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {2: 30, 1: 8, 0: 21, 4: 7, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 30, 1: 8, 0: 21, 4: 7})\n",
            "State:\n",
            "[[-0.44911739  0.06191862]]\n",
            "Meta-Controller reward:\n",
            "-1.67\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.44911739  0.06191862]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38675274  0.06236465]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38675274  0.06236465]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.32438585  0.06236689]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.32438585  0.06236689]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.26242569  0.06196016]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.26242569  0.06196016]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.20122997  0.06119572]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.20122997  0.06119572]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.14109236  0.0601376 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.14109236  0.0601376 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.08223413  0.05885823]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.08223413  0.05885823]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Terminal!\n",
            "Intermediate clusters:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5]\n",
            "Intermediate cluster count:\n",
            "defaultdict(<class 'int'>, {2: 12, 5: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {2: 12, 5: 1})\n",
            "State:\n",
            "[[0.51299153 0.0338328 ]]\n",
            "Meta-Controller reward:\n",
            "-1.13\n",
            "Intrinsic reward:\n",
            "-1\n",
            "Cluster:\n",
            "[0. 0. 0. 0. 0. 1.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.58687948  0.        ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.58540772  0.00147176]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.58540772  0.00147176]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.58247504  0.00293267]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.58247504  0.00293267]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[4, 4, 4, 4, 4, 4, 4, 4]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {4: 8, 3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {4: 8})\n",
            "State:\n",
            "[[-0.497316    0.01388316]]\n",
            "Meta-Controller reward:\n",
            "-1.09\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.497316    0.01388316]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.48262976  0.01468624]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.48262976  0.01468624]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.46725001  0.01537975]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.46725001  0.01537975]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.45129087  0.01595914]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.45129087  0.01595914]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.43486979  0.01642108]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.43486979  0.01642108]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.41810635  0.01676344]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.41810635  0.01676344]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.40112096  0.01698539]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.40112096  0.01698539]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.38403363  0.01708733]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.38403363  0.01708733]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.36696272  0.01707091]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.36696272  0.01707091]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.35002382  0.0169389 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.35002382  0.0169389 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.33332869  0.01669513]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.33332869  0.01669513]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.31698434  0.01634434]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.31698434  0.01634434]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.30109227  0.01589207]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.30109227  0.01589207]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.28574779  0.01534447]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.28574779  0.01534447]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.27103963  0.01470816]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.27103963  0.01470816]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.25704956  0.01399007]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.25704956  0.01399007]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.24385226  0.0131973 ]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.24385226  0.0131973 ]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.2315153   0.01233696]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.2315153   0.01233696]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.2200992  0.0114161]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.2200992  0.0114161]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.20965763  0.01044157]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.20965763  0.01044157]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.20023763  0.00941999]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.20023763  0.00941999]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.19187997  0.00835766]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.19187997  0.00835766]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.18661942  0.00526055]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.18661942  0.00526055]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n",
            "Subgoal completed!\n",
            "Intermediate Clusters:\n",
            "[]\n",
            "Intermediate Cluster Count:\n",
            "defaultdict(<class 'int'>, {3: 1})\n",
            "Intermediate non-beginning cluster count:\n",
            "defaultdict(<class 'int'>, {})\n",
            "State:\n",
            "[[-0.18447719  0.00214223]]\n",
            "Meta-Controller reward:\n",
            "-1.01\n",
            "Intrinsic reward:\n",
            "1\n",
            "Cluster:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "\n",
            "\n",
            "Current State:\n",
            "[[-0.18447719  0.00214223]]\n",
            "Current Meta-Controller State:\n",
            "[0. 0. 0. 1. 0. 0.]\n",
            "Current subgoal picked:\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### make_plots.py"
      ],
      "metadata": {
        "id": "6DxtZcUUXEMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 4\n",
        "directory = 'experiments_final/experiment_logs/Cheating_Epsilon_Decay_Faster/'\n",
        "\n",
        "# 6 clusters\n",
        "sub_dirs = ['_agent_type_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False', '_agent_type_h_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False']\n",
        "# sub_dirs = ['_agent_type_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False', '_agent_type_h_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_True_memory_size_5_pretrain_controller_False']\n",
        "# sub_dirs = ['_agent_type_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False', '_agent_type_h_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_True_memory_size_10_pretrain_controller_False']\n",
        "\n",
        "# 10 clusters\n",
        "# sub_dirs = ['_agent_type_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False', '_agent_type_h_dqn_num_clusters_10_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False']\n",
        "# sub_dirs = ['_agent_type_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False', '_agent_type_h_dqn_num_clusters_10_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_True_memory_size_5_pretrain_controller_False']\n",
        "# sub_dirs = ['_agent_type_dqn_num_clusters_6_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_False_memory_size_5_pretrain_controller_False', '_agent_type_h_dqn_num_clusters_10_use_extra_travel_penalty_False_use_extra_bit_False_use_controller_dqn_False_use_intrinsic_timeout_False_use_memory_True_memory_size_10_pretrain_controller_False']\n",
        "\n",
        "color_index = 0\n",
        "colors = ['r', 'g', 'b']\n",
        "for sub_dir in sub_dirs:\n",
        "    mean_rewards = {}\n",
        "    train_steps = {}\n",
        "    for i in range(4):\n",
        "        full_dir = directory + sub_dir + '_run_number_' + str(i + 1)\n",
        "\n",
        "        f = open(full_dir + str('/log.txt'), 'r')\n",
        "        lines = f.readlines()\n",
        "        mean_rewards_i = []\n",
        "        train_steps_i = []\n",
        "        for j in range(min(len(lines), 300)):\n",
        "            line = lines[j]\n",
        "            line = line.split(' ')\n",
        "            mean_rewards_i.append(float(line[2]))\n",
        "\n",
        "        mean_rewards[i] = mean_rewards_i\n",
        "        # print(mean_rewards[i])\n",
        "        print(len(mean_rewards_i))\n",
        "\n",
        "        for j in range(len(mean_rewards_i)):\n",
        "            with open(full_dir + '/eval_rewards_' + str(j), 'rb') as data_file:\n",
        "                dump_dict = pickle.load(data_file)\n",
        "                train_steps_i.append(dump_dict['train_step'])\n",
        "\n",
        "        train_steps[i] = train_steps_i\n",
        "\n",
        "    # interpolate means to 5000 step intervals\n",
        "    interp_data = [[] for _ in range(4)]\n",
        "    for task_id in range(4):\n",
        "        l = 0\n",
        "        for i in range(0, 700000, 5000):\n",
        "            while train_steps[task_id][l+1] < i:  # step count on left <= i\n",
        "                l += 1\n",
        "            step_l = train_steps[task_id][l]\n",
        "            step_r = train_steps[task_id][l+1]\n",
        "            mean_l = np.mean(mean_rewards[task_id][l])\n",
        "            mean_r = np.mean(mean_rewards[task_id][l+1])\n",
        "            interp = (i - step_l) * mean_r + (step_r - i) * mean_l\n",
        "            interp /= (step_r - step_l)\n",
        "            interp_data[task_id].append((i, interp))\n",
        "\n",
        "    # print(interp_data)\n",
        "\n",
        "    means_0 = [m for (t, m) in interp_data[0]]\n",
        "    means_1 = [m for (t, m) in interp_data[1]]\n",
        "    means_2 = [m for (t, m) in interp_data[2]]\n",
        "    means_3 = [m for (t, m) in interp_data[3]]\n",
        "    steps = [t for (t, m) in interp_data[0]]\n",
        "    means_arr = [[means_0[k], means_1[k], means_2[k], means_3[k]] for k in range(len(means_0))]\n",
        "    means = [np.mean(means_arr[k]) for k in range(len(means_0))]\n",
        "\n",
        "    # mean_reward_arrs = [mean_rewards[i] for i in mean_rewards]\n",
        "    # means = []\n",
        "    # num_to_plot = 300\n",
        "    # for i in range(num_to_plot):\n",
        "        # means.append((\n",
        "        #    mean_rewards[0][i] + mean_rewards[1][i] + mean_rewards[2][i] + mean_rewards[3][i] + mean_rewards[4][i]) / 5.0)\n",
        "        # means.append((mean_rewards[index][i]))\n",
        "\n",
        "\n",
        "\n",
        "    # plt.scatter(np.arange(num_to_plot), means, c=colors[color_index])\n",
        "    # plt.plot(steps, means, c=colors[color_index])\n",
        "    plt.fill_between(\n",
        "        steps, [np.percentile(r, 10) for r in means_arr], [np.percentile(\n",
        "            r, 90) for r in means_arr], facecolor=colors[color_index], alpha=0.2)\n",
        "    color_index += 1\n",
        "\n",
        "plt.savefig('plot_' + str(index) + '.png')"
      ],
      "metadata": {
        "id": "xeQyZCJ_XGRt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}